{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 117\u001b[0m\n\u001b[0;32m    115\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    116\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 117\u001b[0m x_recon, z_mean, z_log_var \u001b[38;5;241m=\u001b[39m \u001b[43mcvae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(x, x_recon, z_mean, z_log_var)\n\u001b[0;32m    119\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\benny\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\benny\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 86\u001b[0m, in \u001b[0;36mCVAE.forward\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y):\n\u001b[1;32m---> 86\u001b[0m     z_mean, z_log_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(z_mean, z_log_var)\n\u001b[0;32m     88\u001b[0m     x_recon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(z, y)\n",
      "File \u001b[1;32mc:\\Users\\benny\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\benny\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 43\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# 連接影像特徵和類別嵌入向量\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[0;32m     46\u001b[0m z_mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_mean(x)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 3"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "PATH = '/content/CVAE/generated_images'\n",
    "epochs = 20\n",
    "\n",
    "# 定義CVAE模型\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim, input_size=32, num_classes=10):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, stride=2, padding=1)  # 32 -> 16\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, stride=2, padding=1) # 16 -> 8\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, stride=2, padding=1) # 8 -> 4\n",
    "        self.conv4 = nn.Conv2d(64, 128, 3, stride=2, padding=1) # 4 -> 2\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        final_size = input_size // 2**4  # 32 -> 16 -> 8 -> 4 -> 2\n",
    "        self.fc1 = nn.Linear(final_size * final_size * 128 + num_classes, 256)  # 將類別標籤嵌入到中間層\n",
    "        self.fc_mean = nn.Linear(256, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(256, latent_dim)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        # 嵌入類別標籤\n",
    "        y = y.view(y.size(0), -1)  # 攤平類別標籤\n",
    "        y = torch.nn.functional.one_hot(y, num_classes=10)  # 將類別標籤轉換為獨熱編碼\n",
    "        y = y.float()  # 轉換為浮點數型態\n",
    "        \n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # 連接影像特徵和類別嵌入向量\n",
    "        x = torch.cat((x, y), dim=1)\n",
    "        \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        z_mean = self.fc_mean(x)\n",
    "        z_log_var = self.fc_log_var(x)\n",
    "        return z_mean, z_log_var\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_size=32, num_classes=10):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        final_size = output_size // 2**4  # 32 -> 16 -> 8 -> 4 -> 2\n",
    "        self.fc1 = nn.Linear(latent_dim + num_classes, final_size * final_size * 128)  # 將類別標籤嵌入到中間層\n",
    "        self.conv1 = nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1) # 2 -> 4\n",
    "        self.conv2 = nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1)  # 4 -> 8\n",
    "        self.conv3 = nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1)  # 8 -> 16\n",
    "        self.conv4 = nn.ConvTranspose2d(16, 3, 3, stride=2, padding=1, output_padding=1)   # 16 -> 32\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        # 嵌入類別標籤\n",
    "        y = y.view(y.size(0), -1)  # 攤平類別標籤\n",
    "        y = torch.nn.functional.one_hot(y, num_classes=10)  # 將類別標籤轉換為獨熱編碼\n",
    "        y = y.float()  # 轉換為浮點數型態\n",
    "        \n",
    "        # 連接潛在向量和類別嵌入向量\n",
    "        z = torch.cat((z, y), dim=1)\n",
    "        \n",
    "        x = torch.relu(self.fc1(z))\n",
    "        x = x.view(-1, 128, 2, 2)  # 修改這裡以匹配最終特徵圖大小\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.sigmoid(self.conv4(x))\n",
    "        return x\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        z_mean, z_log_var = self.encoder(x, y)\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "        x_recon = self.decoder(z, y)\n",
    "        return x_recon, z_mean, z_log_var\n",
    "\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        epsilon = torch.randn_like(log_var)\n",
    "        return mean + torch.exp(0.5 * log_var) * epsilon\n",
    "\n",
    "# 載入CIFAR-10資料集\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# 訓練CVAE模型\n",
    "latent_dim = 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cvae = CVAE(latent_dim).to(device)\n",
    "optimizer = optim.Adam(cvae.parameters(), lr=1e-3)\n",
    "\n",
    "def loss_function(x, x_recon, z_mean, z_log_var):\n",
    "    recon_loss = nn.functional.mse_loss(x_recon, x, reduction='sum')\n",
    "    kl_divergence = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())\n",
    "    return recon_loss + kl_divergence\n",
    "\n",
    "# 訓練迴圈\n",
    "for epoch in range(epochs):\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        x_recon, z_mean, z_log_var = cvae(x, y)\n",
    "        loss = loss_function(x, x_recon, z_mean, z_log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "# 選擇10個不同類別的圖片索引\n",
    "indices = []\n",
    "classes = np.arange(10)\n",
    "for i in classes:\n",
    "    idx = np.where(np.array(train_dataset.targets) == i)[0][0]\n",
    "    indices.append(idx)\n",
    "\n",
    "# 生成每個類別的 1000 張影像\n",
    "num_samples_per_class = 1000\n",
    "generated_images = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in classes:\n",
    "        for _ in range(num_samples_per_class):\n",
    "            z = torch.randn(1, latent_dim).to(device)\n",
    "            y = torch.tensor([i]).to(device)\n",
    "            x_recon = cvae.decoder(z, y)\n",
    "            generated_images.append(x_recon.squeeze(0))\n",
    "\n",
    "\n",
    "# 定義10個類別的顏色\n",
    "name = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# 顯示每個類別的前5張圖片\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(10):\n",
    "    for j in range(5):\n",
    "        idx = i * 1000 + j\n",
    "        image = generated_images[idx].cpu().numpy().transpose(1, 2, 0)\n",
    "        plt.subplot(10, 10, i * 10 + j + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        if j == 0:\n",
    "            plt.title(name[i])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 創建一個資料夾來保存生成的圖片\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "\n",
    "# 將生成的圖片保存到硬碟\n",
    "for i, img in enumerate(generated_images):\n",
    "    # 將圖片數據轉換為 0-255 範圍\n",
    "    img = (img * 255).clamp(0, 255).byte()\n",
    "    img = img.permute(1, 2, 0)  # 將張量的形狀從 (channels, height, width) 改為 (height, width, channels)\n",
    "    img = Image.fromarray(img.cpu().numpy())\n",
    "    img.save(f'{PATH}/image_{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "PATH = '/content/CVAE/generated_images'\n",
    "epochs = 20\n",
    "latent_dim = 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 定義 CVAE 模型 (略過這部分代碼，假設你已經定義好模型)\n",
    "\n",
    "# 載入 CIFAR-10 測試集\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# 加載預訓練的 Inception V3 模型\n",
    "inception_model = models.inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "inception_model.eval()\n",
    "\n",
    "# 定義一個函數來提取特徵\n",
    "def get_features(images, model):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, images.size(0), 128):  # 使用合適的 batch size 進行處理\n",
    "            batch = images[i:i+128].to(device)\n",
    "            batch = nn.functional.interpolate(batch, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "            feature = model(batch).detach().cpu().numpy()\n",
    "            features.append(feature)\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    return features\n",
    "\n",
    "# 提取生成圖片的特徵\n",
    "gen_images = torch.stack(generated_images)  # 保證生成圖片是四維的 (N, 3, 32, 32)\n",
    "gen_features = get_features(gen_images, inception_model)\n",
    "\n",
    "# 提取真實圖片的特徵\n",
    "real_images = []\n",
    "for x, _ in test_loader:\n",
    "    real_images.append(x)\n",
    "real_images = torch.cat(real_images, dim=0)  # 保證真實圖片是四維的 (N, 3, 32, 32)\n",
    "real_features = get_features(real_images, inception_model)\n",
    "\n",
    "# 計算 FID\n",
    "def calculate_fid(real_features, gen_features):\n",
    "    mu1 = np.mean(real_features, axis=0)\n",
    "    mu2 = np.mean(gen_features, axis=0)\n",
    "    sigma1 = np.cov(real_features, rowvar=False)\n",
    "    sigma2 = np.cov(gen_features, rowvar=False)\n",
    "\n",
    "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    \n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid\n",
    "\n",
    "fid_score = calculate_fid(real_features, gen_features)\n",
    "print(f\"FID score: {fid_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
